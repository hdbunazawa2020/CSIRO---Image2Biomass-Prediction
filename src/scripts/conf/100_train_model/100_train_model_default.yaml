# ==========
# general
# ==========
exp: 100_train_exp011   # TODO: 実験連番
seed: 1129
debug: false
device: cuda
use_amp: true
num_workers: 12
pin_memory: true
persistent_workers: true
# WandB
use_wandb: true
competition: Csiro-Image2BiomassPrediction
author: hidebu
wandb_log_interval_steps: 100
# DDP
ddp:
  enabled: true
  backend: nccl
  find_unused_parameters: false

# ==========
# paths
# ==========
input_dir: /mnt/nfs/home/hidebu/study/CSIRO---Image2Biomass-Prediction/data/raw
pp_dir: /mnt/nfs/home/hidebu/study/CSIRO---Image2Biomass-Prediction/data/processed
preprocess_ver: 000_preprocess_ver00
pivot_csv_name: df_pivot.csv

output_dir: /mnt/nfs/home/hidebu/study/CSIRO---Image2Biomass-Prediction/experiments

# ==========
# CV
# ==========
fold_col: Fold
folds: [0,1,2,3,4]          # [0,1,2,3,4] など

# =======================================
# targets (order MUST match metric.py)
# =======================================
target_cols:
  - Dry_Green_g
  - Dry_Clover_g
  - Dry_Dead_g
  - GDM_g
  - Dry_Total_g

use_log1p_target: true

# =======================
# image / augmentation
# =======================
img_size: 288 # 224
normalize:
  mean: [0.485, 0.456, 0.406]
  std:  [0.229, 0.224, 0.225]

augment:
  train:
    hflip_p: 0.25 # 0.5
    vflip_p: 0.5
    rotate_limit: 0 # 10
    shift_scale_rotate_p: 0.5 # 0.2
    color_jitter_p: 0.2 # 0.2
  valid:
    # validは resize + normalize のみ
    dummy: 0

# ==========================
# MixUp / CutMix (train only)
# ==========================
mixing:
  enabled: false # true
  p: 0.14075341328864174  # 小さめ確率（まずは 0.10〜0.20 が無難）
  mode: cutmix   # mixup | cutmix | mixup_cutmix
  mixup_alpha: 1.0     # Beta(alpha, alpha)
  cutmix_alpha: 1.0    # Beta(alpha, alpha)
  switch_prob: 0.5     # mode=mixup_cutmix のとき CutMix を選ぶ確率

# ==========
# model
# ==========
model:
  backbone: convnext_small            # ✅ ConvNeXt small / convnext_base
  # backbone: swin_tiny               # ✅ alias → swin_tiny_patch4_window7_224
  # backbone: swin_tiny_patch4_window7_224
  # backbone: efficientnetv2_s        # ✅ alias → tf_efficientnetv2_s
  # backbone: tf_efficientnetv2_s
  pretrained: true
  in_chans: 3
  drop_rate: 0.0
  drop_path_rate: 0.1
  head_dropout: 0.0 # 0.1

# ==========
# training
# ==========
train:
  epochs: 200
  batch_size: 32
  grad_accum_steps: 1
  max_norm: 3.0
  log_interval: 50
  val_interval: 1

early_stopping:
  enabled: true
  patience: 1000
  min_delta: 0.0

ema:
  enabled: false # true
  decay: 0.0 # 0.95

# =========================
# optimizer & scheduler
# =========================
optimizer:
  name: adamw
  base_lr: 0.00001136562991924597 # 1e-4 #  1e-4
  weight_decay: 0.00023433482410557073 # 0.05 # 1e-5
  betas: [0.9, 0.999]

scheduler:
  name: warmup_cosine
  base_lr: 0.00001136562991924597 # 1e-4
  max_lr: 0.00001136562991924597 # 1e-4 #  5e-4
  min_lr: 0.00001136562991924597 # 1e-4 # 1e-6
  warmup_ratio: 0.05   # warmup_steps が null のときに使う
  warmup_steps: null
  total_steps: null

# ===================================================
# loss / metric weights (same order as target_cols)
# ===================================================
# loss:
#   name: weighted_mse
#   weights: [0.1, 0.1, 0.1, 0.2, 0.5]
loss:
  name: mixed_log_raw
  weights: [0.1, 0.1, 0.1, 0.2, 0.5]
  # ★ まずは小さめから（rawのスケールが大きいので）
  alpha_raw: 0.05
  alpha_warmup_epochs: 0 # 10
  # まずはMSE（高値域を強く詰めたい）
  raw_loss: l1 # mse
  raw_huber_beta: 10 # 5.0  # raw_loss=huberの時だけ有効
  log_clip_min: -20.0
  log_clip_max: 20.0
# raw_loss: huber
# raw_huber_beta: 10.0
# alpha_raw: 0.1

metric:
  name: weighted_r2
  weights: [0.1, 0.1, 0.1, 0.2, 0.5]